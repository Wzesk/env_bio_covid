Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 4
Rules claiming more threads will be scaled down.
Conda environments: ignored
Job stats:
job                                  count    min threads    max threads
---------------------------------  -------  -------------  -------------
add_branch_labels                        1              1              1
adjust_metadata_regions                  1              1              1
align                                    3              4              4
all                                      1              1              1
ancestral                                1              1              1
build_align                              1              4              4
calculate_epiweeks                       1              1              1
clade_files                              1              1              1
clades                                   1              1              1
colors                                   1              1              1
combine_input_metadata                   1              1              1
combine_samples                          1              1              1
combine_sequences_for_subsampling        1              1              1
diagnostic                               3              1              1
distances                                1              1              1
emerging_lineages                        1              1              1
export                                   1              1              1
filter                                   3              1              1
finalize                                 1              1              1
include_hcov19_prefix                    1              1              1
index_sequences                          1              1              1
logistic_growth                          1              1              1
mask                                     1              1              1
mutational_fitness                       1              1              1
priority_score                           1              1              1
proximity_score                          1              1              1
recency                                  1              1              1
refine                                   1              1              1
rename_emerging_lineages                 1              1              1
sanitize_metadata                        3              1              1
subsample                                2              1              1
tip_frequencies                          1              1              1
traits                                   1              1              1
translate                                1              1              1
tree                                     1              4              4
total                                   44              1              4

Select jobs to execute...

[Thu Nov 25 10:44:30 2021]
Job 12: 
        Aligning sequences to defaults/reference_seq.fasta
            - gaps relative to reference are considered real
        


        python3 scripts/sanitize_sequences.py             --sequences data/gisaid_nyc_sequences.fasta             --strip-prefixes hCoV-19/ SARS-CoV-2/             --output /dev/stdout 2> logs/sanitize_sequences_new_york.txt             | nextalign             --jobs=4             --reference defaults/reference_seq.fasta             --genemap defaults/annotation.gff             --genes ORF1a,ORF1b,S,ORF3a,E,M,ORF6,ORF7a,ORF7b,ORF8,N,ORF9b             --sequences /dev/stdin             --output-dir results/translations             --output-basename seqs_new_york             --output-fasta results/aligned_new_york.fasta             --output-insertions results/insertions_new_york.tsv > logs/align_new_york.txt 2>&1;
        xz -2 results/aligned_new_york.fasta;
        xz -2 results/translations/seqs_new_york*.fasta
        
[Thu Nov 25 10:45:17 2021]
Finished job 12.
1 of 44 steps (2%) done
Select jobs to execute...

[Thu Nov 25 10:45:17 2021]
Job 16: 
        Aligning sequences to defaults/reference_seq.fasta
            - gaps relative to reference are considered real
        


        python3 scripts/sanitize_sequences.py             --sequences data/hcov_north-america.fasta             --strip-prefixes hCoV-19/ SARS-CoV-2/             --output /dev/stdout 2> logs/sanitize_sequences_north_america.txt             | nextalign             --jobs=4             --reference defaults/reference_seq.fasta             --genemap defaults/annotation.gff             --genes ORF1a,ORF1b,S,ORF3a,E,M,ORF6,ORF7a,ORF7b,ORF8,N,ORF9b             --sequences /dev/stdin             --output-dir results/translations             --output-basename seqs_north_america             --output-fasta results/aligned_north_america.fasta             --output-insertions results/insertions_north_america.tsv > logs/align_north_america.txt 2>&1;
        xz -2 results/aligned_north_america.fasta;
        xz -2 results/translations/seqs_north_america*.fasta
        
[Thu Nov 25 10:46:02 2021]
Finished job 16.
2 of 44 steps (5%) done
Select jobs to execute...

[Thu Nov 25 10:46:02 2021]
Job 20: 
        Aligning sequences to defaults/reference_seq.fasta
            - gaps relative to reference are considered real
        


        python3 scripts/sanitize_sequences.py             --sequences data/wuhan_reference.sequences.fasta             --strip-prefixes hCoV-19/ SARS-CoV-2/             --output /dev/stdout 2> logs/sanitize_sequences_references.txt             | nextalign             --jobs=4             --reference defaults/reference_seq.fasta             --genemap defaults/annotation.gff             --genes ORF1a,ORF1b,S,ORF3a,E,M,ORF6,ORF7a,ORF7b,ORF8,N,ORF9b             --sequences /dev/stdin             --output-dir results/translations             --output-basename seqs_references             --output-fasta results/aligned_references.fasta             --output-insertions results/insertions_references.tsv > logs/align_references.txt 2>&1;
        xz -2 results/aligned_references.fasta;
        xz -2 results/translations/seqs_references*.fasta
        
[Thu Nov 25 10:46:22 2021]
Finished job 20.
3 of 44 steps (7%) done
Select jobs to execute...

[Thu Nov 25 10:46:23 2021]
rule sanitize_metadata:
    input: data/gisaid_nyc_metadata.tsv
    output: results/sanitized_metadata_new_york.tsv.xz
    log: logs/sanitize_metadata_new_york.txt
    jobid: 13
    benchmark: benchmarks/sanitize_metadata_new_york.txt
    wildcards: origin=new_york
    resources: tmpdir=/tmp, mem_mb=2000


        python3 scripts/sanitize_metadata.py             --metadata data/gisaid_nyc_metadata.tsv             --metadata-id-columns strain name 'Virus name'             --database-id-columns 'Accession ID' gisaid_epi_isl genbank_accession             --parse-location-field Location             --rename-fields 'Virus name=strain' Type=type 'Accession ID=gisaid_epi_isl' 'Collection date=date' 'Additional location information=additional_location_information' 'Sequence length=length' Host=host 'Patient age=patient_age' Gender=sex Clade=GISAID_clade 'Pango lineage=pango_lineage' pangolin_lineage=pango_lineage Lineage=pango_lineage 'Pangolin version=pangolin_version' Variant=variant 'AA Substitutions=aa_substitutions' aaSubstitutions=aa_substitutions 'Submission date=date_submitted' 'Is reference?=is_reference' 'Is complete?=is_complete' 'Is high coverage?=is_high_coverage' 'Is low coverage?=is_low_coverage' N-Content=n_content GC-Content=gc_content             --strip-prefixes hCoV-19/ SARS-CoV-2/                          --output results/sanitized_metadata_new_york.tsv.xz 2>&1 | tee logs/sanitize_metadata_new_york.txt
        

[Thu Nov 25 10:46:23 2021]
rule sanitize_metadata:
    input: data/hcov_north-america.tsv
    output: results/sanitized_metadata_north_america.tsv.xz
    log: logs/sanitize_metadata_north_america.txt
    jobid: 17
    benchmark: benchmarks/sanitize_metadata_north_america.txt
    wildcards: origin=north_america
    resources: tmpdir=/tmp, mem_mb=2000


        python3 scripts/sanitize_metadata.py             --metadata data/hcov_north-america.tsv             --metadata-id-columns strain name 'Virus name'             --database-id-columns 'Accession ID' gisaid_epi_isl genbank_accession             --parse-location-field Location             --rename-fields 'Virus name=strain' Type=type 'Accession ID=gisaid_epi_isl' 'Collection date=date' 'Additional location information=additional_location_information' 'Sequence length=length' Host=host 'Patient age=patient_age' Gender=sex Clade=GISAID_clade 'Pango lineage=pango_lineage' pangolin_lineage=pango_lineage Lineage=pango_lineage 'Pangolin version=pangolin_version' Variant=variant 'AA Substitutions=aa_substitutions' aaSubstitutions=aa_substitutions 'Submission date=date_submitted' 'Is reference?=is_reference' 'Is complete?=is_complete' 'Is high coverage?=is_high_coverage' 'Is low coverage?=is_low_coverage' N-Content=n_content GC-Content=gc_content             --strip-prefixes hCoV-19/ SARS-CoV-2/                          --output results/sanitized_metadata_north_america.tsv.xz 2>&1 | tee logs/sanitize_metadata_north_america.txt
        

[Thu Nov 25 10:46:23 2021]
rule clade_files:
    input: defaults/clades.tsv
    output: results/new_york/clades.tsv
    jobid: 35
    benchmark: benchmarks/clade_files_new_york.txt
    wildcards: build_name=new_york
    resources: tmpdir=/tmp


        cat defaults/clades.tsv > results/new_york/clades.tsv
        

[Thu Nov 25 10:46:23 2021]
rule sanitize_metadata:
    input: data/wuhan_reference.metadata.tsv
    output: results/sanitized_metadata_references.tsv.xz
    log: logs/sanitize_metadata_references.txt
    jobid: 21
    benchmark: benchmarks/sanitize_metadata_references.txt
    wildcards: origin=references
    resources: tmpdir=/tmp, mem_mb=2000


        python3 scripts/sanitize_metadata.py             --metadata data/wuhan_reference.metadata.tsv             --metadata-id-columns strain name 'Virus name'             --database-id-columns 'Accession ID' gisaid_epi_isl genbank_accession             --parse-location-field Location             --rename-fields 'Virus name=strain' Type=type 'Accession ID=gisaid_epi_isl' 'Collection date=date' 'Additional location information=additional_location_information' 'Sequence length=length' Host=host 'Patient age=patient_age' Gender=sex Clade=GISAID_clade 'Pango lineage=pango_lineage' pangolin_lineage=pango_lineage Lineage=pango_lineage 'Pangolin version=pangolin_version' Variant=variant 'AA Substitutions=aa_substitutions' aaSubstitutions=aa_substitutions 'Submission date=date_submitted' 'Is reference?=is_reference' 'Is complete?=is_complete' 'Is high coverage?=is_high_coverage' 'Is low coverage?=is_low_coverage' N-Content=n_content GC-Content=gc_content             --strip-prefixes hCoV-19/ SARS-CoV-2/                          --output results/sanitized_metadata_references.tsv.xz 2>&1 | tee logs/sanitize_metadata_references.txt
        
[Thu Nov 25 10:46:23 2021]
Finished job 35.
4 of 44 steps (9%) done
[Thu Nov 25 10:46:43 2021]
Finished job 21.
5 of 44 steps (11%) done
Select jobs to execute...

[Thu Nov 25 10:46:43 2021]
Job 22: Scanning metadata results/sanitized_metadata_references.tsv.xz for problematic sequences. Removing sequences with >20 deviation from the clock and with more than 1.


        python3 scripts/diagnostic.py             --metadata results/sanitized_metadata_references.tsv.xz             --clock-filter 20             --rare-mutations 100             --clock-plus-rare 100             --snp-clusters 1             --output-exclusion-list results/to-exclude_references.txt 2>&1 | tee logs/diagnostics_references.txt
        
[Thu Nov 25 10:46:43 2021]
Finished job 13.
6 of 44 steps (14%) done
Select jobs to execute...

[Thu Nov 25 10:46:43 2021]
Job 14: Scanning metadata results/sanitized_metadata_new_york.tsv.xz for problematic sequences. Removing sequences with >20 deviation from the clock and with more than 1.


        python3 scripts/diagnostic.py             --metadata results/sanitized_metadata_new_york.tsv.xz             --clock-filter 20             --rare-mutations 100             --clock-plus-rare 100             --snp-clusters 1             --output-exclusion-list results/to-exclude_new_york.txt 2>&1 | tee logs/diagnostics_new_york.txt
        
[Thu Nov 25 10:46:43 2021]
Finished job 17.
7 of 44 steps (16%) done
Select jobs to execute...

[Thu Nov 25 10:46:44 2021]
Job 24: 
        Combining metadata files results/sanitized_metadata_new_york.tsv.xz results/sanitized_metadata_north_america.tsv.xz results/sanitized_metadata_references.tsv.xz -> results/combined_metadata.tsv.xz and adding columns to represent origin
        


        python3 scripts/combine_metadata.py --metadata results/sanitized_metadata_new_york.tsv.xz results/sanitized_metadata_north_america.tsv.xz results/sanitized_metadata_references.tsv.xz --origins new_york north_america references --output results/combined_metadata.tsv.xz 2>&1 | tee logs/combine_input_metadata.txt
        

[Thu Nov 25 10:46:44 2021]
Job 18: Scanning metadata results/sanitized_metadata_north_america.tsv.xz for problematic sequences. Removing sequences with >20 deviation from the clock and with more than 1.


        python3 scripts/diagnostic.py             --metadata results/sanitized_metadata_north_america.tsv.xz             --clock-filter 20             --rare-mutations 100             --clock-plus-rare 100             --snp-clusters 1             --output-exclusion-list results/to-exclude_north_america.txt 2>&1 | tee logs/diagnostics_north_america.txt
        
[Thu Nov 25 10:46:52 2021]
Finished job 22.
8 of 44 steps (18%) done
Select jobs to execute...

[Thu Nov 25 10:46:52 2021]
Job 19: 
        Filtering alignment results/aligned_references.fasta.xz -> results/filtered_references.fasta.xz
          - excluding strains in defaults/exclude.txt results/to-exclude_references.txt
          - including strains in defaults/include.txt
          - min length: 27000
        


        augur filter             --sequences results/aligned_references.fasta.xz             --metadata results/sanitized_metadata_references.tsv.xz             --include defaults/include.txt             --max-date 2021-11-26             --min-date 2019.74             --exclude-ambiguous-dates-by any             --exclude defaults/exclude.txt results/to-exclude_references.txt             --exclude-where division='USA'            --min-length 27000             --output results/filtered_references.fasta 2>&1 | tee logs/filtered_references.txt;
        xz -2 results/filtered_references.fasta
        
[Thu Nov 25 10:46:55 2021]
Finished job 14.
9 of 44 steps (20%) done
Select jobs to execute...

[Thu Nov 25 10:46:55 2021]
Job 11: 
        Filtering alignment results/aligned_new_york.fasta.xz -> results/filtered_new_york.fasta.xz
          - excluding strains in defaults/exclude.txt results/to-exclude_new_york.txt
          - including strains in defaults/include.txt
          - min length: 27000
        


        augur filter             --sequences results/aligned_new_york.fasta.xz             --metadata results/sanitized_metadata_new_york.tsv.xz             --include defaults/include.txt             --max-date 2021-11-26             --min-date 2019.74             --exclude-ambiguous-dates-by any             --exclude defaults/exclude.txt results/to-exclude_new_york.txt             --exclude-where division='USA'            --min-length 27000             --output results/filtered_new_york.fasta 2>&1 | tee logs/filtered_new_york.txt;
        xz -2 results/filtered_new_york.fasta
        
[Thu Nov 25 10:46:56 2021]
Finished job 18.
10 of 44 steps (23%) done
Select jobs to execute...

[Thu Nov 25 10:46:56 2021]
Job 15: 
        Filtering alignment results/aligned_north_america.fasta.xz -> results/filtered_north_america.fasta.xz
          - excluding strains in defaults/exclude.txt results/to-exclude_north_america.txt
          - including strains in defaults/include.txt
          - min length: 27000
        


        augur filter             --sequences results/aligned_north_america.fasta.xz             --metadata results/sanitized_metadata_north_america.tsv.xz             --include defaults/include.txt             --max-date 2021-11-26             --min-date 2019.74             --exclude-ambiguous-dates-by any             --exclude defaults/exclude.txt results/to-exclude_north_america.txt             --exclude-where division='USA'            --min-length 27000             --output results/filtered_north_america.fasta 2>&1 | tee logs/filtered_north_america.txt;
        xz -2 results/filtered_north_america.fasta
        
[Thu Nov 25 10:47:03 2021]
Finished job 24.
11 of 44 steps (25%) done
[Thu Nov 25 10:47:09 2021]
Finished job 19.
12 of 44 steps (27%) done
[Thu Nov 25 10:47:11 2021]
Finished job 11.
13 of 44 steps (30%) done
[Thu Nov 25 10:47:12 2021]
Finished job 15.
14 of 44 steps (32%) done
Select jobs to execute...

[Thu Nov 25 10:47:12 2021]
Job 10: 
        Combine and deduplicate aligned & filtered FASTAs from multiple origins in preparation for subsampling.
        


        python3 scripts/sanitize_sequences.py                 --sequences results/filtered_new_york.fasta.xz results/filtered_north_america.fasta.xz results/filtered_references.fasta.xz                 --strip-prefixes hCoV-19/ SARS-CoV-2/                                  --output /dev/stdout                 | xz -c -2 > results/combined_sequences_for_subsampling.fasta.xz
        
[Thu Nov 25 10:47:34 2021]
Finished job 10.
15 of 44 steps (34%) done
Select jobs to execute...

[Thu Nov 25 10:47:34 2021]
Job 23: 
        Index sequence composition for faster filtering.
        


        augur index             --sequences results/combined_sequences_for_subsampling.fasta.xz             --output results/combined_sequence_index.tsv.xz 2>&1 | tee logs/index_sequences.txt
        
[Thu Nov 25 10:47:58 2021]
Finished job 23.
16 of 44 steps (36%) done
Select jobs to execute...

[Thu Nov 25 10:47:58 2021]
Job 25: 
        Subsample all sequences by 'focal' scheme for build 'new_york' with the following parameters:

         - group by: 
         - sequences per group: 
         - subsample max sequences: 
         - min-date: 
         - max-date: 
         - 
         - exclude: 
         - include: 
         - query: --query "division == 'New York City'"
         - priority: 
        


        augur filter             --sequences results/combined_sequences_for_subsampling.fasta.xz             --metadata results/combined_metadata.tsv.xz             --sequence-index results/combined_sequence_index.tsv.xz             --include defaults/include.txt             --exclude defaults/exclude.txt                                                                 --query "division == 'New York City'"                                                                                           --output results/new_york/sample-focal.fasta             --output-strains results/new_york/sample-focal.txt 2>&1 | tee logs/subsample_new_york_focal.txt
        
[Thu Nov 25 10:48:21 2021]
Finished job 25.
17 of 44 steps (39%) done
Select jobs to execute...

[Thu Nov 25 10:48:21 2021]
Job 28: 
        determine priority for inclusion in as phylogenetic context by
        genetic similiarity to sequences in focal set for build 'new_york'.
        


        python3 scripts/get_distance_to_focal_set.py             --reference defaults/reference_seq.fasta             --alignment results/combined_sequences_for_subsampling.fasta.xz             --focal-alignment results/new_york/sample-focal.fasta             --ignore-seqs Wuhan/Hu-1/2019             --chunk-size 10000             --output results/new_york/proximity_focal.tsv 2>&1 | tee logs/subsampling_proximity_new_york_focal.txt
        
[Thu Nov 25 10:48:42 2021]
Finished job 28.
18 of 44 steps (41%) done
Select jobs to execute...

[Thu Nov 25 10:48:42 2021]
rule priority_score:
    input: results/new_york/proximity_focal.tsv, results/combined_sequence_index.tsv.xz
    output: results/new_york/priorities_focal.tsv
    jobid: 27
    benchmark: benchmarks/priority_score_new_york_focal.txt
    wildcards: build_name=new_york, focus=focal
    resources: tmpdir=/tmp


        python3 scripts/priorities.py             --sequence-index results/combined_sequence_index.tsv.xz             --proximities results/new_york/proximity_focal.tsv             --crowding-penalty 0.1             --Nweight 0.003             --output results/new_york/priorities_focal.tsv 2>&1 | tee 
        
[Thu Nov 25 10:48:51 2021]
Finished job 27.
19 of 44 steps (43%) done
Select jobs to execute...

[Thu Nov 25 10:48:51 2021]
Job 26: 
        Subsample all sequences by 'contextual' scheme for build 'new_york' with the following parameters:

         - group by: 
         - sequences per group: 
         - subsample max sequences: 
         - min-date: 
         - max-date: 
         - 
         - exclude: 
         - include: 
         - query: --query "division != 'New York City'"
         - priority: --priority results/new_york/priorities_focal.tsv
        


        augur filter             --sequences results/combined_sequences_for_subsampling.fasta.xz             --metadata results/combined_metadata.tsv.xz             --sequence-index results/combined_sequence_index.tsv.xz             --include defaults/include.txt             --exclude defaults/exclude.txt                                                                 --query "division != 'New York City'"                          --priority results/new_york/priorities_focal.tsv                                                                 --output results/new_york/sample-contextual.fasta             --output-strains results/new_york/sample-contextual.txt 2>&1 | tee logs/subsample_new_york_contextual.txt
        
[Thu Nov 25 10:49:12 2021]
Finished job 26.
20 of 44 steps (45%) done
Select jobs to execute...

[Thu Nov 25 10:49:12 2021]
Job 9: 
        Combine and deduplicate FASTAs
        


        augur filter             --sequences results/combined_sequences_for_subsampling.fasta.xz             --sequence-index results/combined_sequence_index.tsv.xz             --metadata results/combined_metadata.tsv.xz             --exclude-all             --include results/new_york/sample-focal.txt results/new_york/sample-contextual.txt             --output-sequences results/new_york/new_york_subsampled_sequences.fasta.xz             --output-metadata results/new_york/new_york_subsampled_metadata.tsv.xz 2>&1 | tee logs/subsample_regions_new_york.txt
        
[Thu Nov 25 10:49:58 2021]
Finished job 9.
21 of 44 steps (48%) done
Select jobs to execute...

[Thu Nov 25 10:49:59 2021]
Job 8: 
        Aligning sequences to defaults/reference_seq.fasta
            - gaps relative to reference are considered real
        


        xz -c -d results/new_york/new_york_subsampled_sequences.fasta.xz | nextalign             --jobs=4             --reference defaults/reference_seq.fasta             --genemap defaults/annotation.gff             --genes ORF1a,ORF1b,S,ORF3a,E,M,ORF6,ORF7a,ORF7b,ORF8,N,ORF9b             --sequences /dev/stdin             --output-dir results/new_york/translations             --output-basename aligned             --output-fasta results/new_york/aligned.fasta             --output-insertions results/new_york/insertions.tsv > logs/align_new_york.txt 2>&1
        
[Thu Nov 25 10:50:42 2021]
Finished job 8.
22 of 44 steps (50%) done
Select jobs to execute...

[Thu Nov 25 10:50:42 2021]
Job 7: 
        Mask bases in alignment results/new_york/aligned.fasta
          - masking 100 from beginning
          - masking 50 from end
          - masking other sites: 21987 21846
        


        python3 scripts/mask-alignment.py             --alignment results/new_york/aligned.fasta             --mask-from-beginning 100             --mask-from-end 50             --mask-sites 21987 21846             --mask-terminal-gaps             --output results/new_york/masked.fasta 2> logs/mask_new_york.txt
        

[Thu Nov 25 10:50:42 2021]
Job 29: 
        Adjusting metadata for build 'new_york'
        


        python3 scripts/adjust_regional_meta.py             --region 'North America'             --metadata results/new_york/new_york_subsampled_metadata.tsv.xz             --output results/new_york/metadata_adjusted.tsv.xz 2>&1 | tee logs/adjust_metadata_regions_new_york.txt
        
[Thu Nov 25 10:50:51 2021]
Finished job 29.
23 of 44 steps (52%) done
Select jobs to execute...

[Thu Nov 25 10:50:51 2021]
rule calculate_epiweeks:
    input: results/new_york/metadata_adjusted.tsv.xz
    output: results/new_york/epiweeks.json
    log: logs/calculate_epiweeks_new_york.txt
    jobid: 42
    benchmark: benchmarks/calculate_epiweeks_new_york.txt
    wildcards: build_name=new_york
    resources: tmpdir=/tmp


        python3 scripts/calculate_epiweek.py             --metadata results/new_york/metadata_adjusted.tsv.xz             --output-node-data results/new_york/epiweeks.json 2>&1 | tee logs/calculate_epiweeks_new_york.txt
        

[Thu Nov 25 10:50:51 2021]
Job 43: Constructing colors file


        python3 scripts/assign-colors.py             --ordering defaults/color_ordering.tsv             --color-schemes defaults/color_schemes.tsv             --output results/new_york/colors.tsv             --metadata results/new_york/metadata_adjusted.tsv.xz 2>&1 | tee logs/colors_new_york.txt
        

[Thu Nov 25 10:50:51 2021]
Job 36: Use metadata on submission date to construct submission recency field


        python3 scripts/construct-recency-from-submission-date.py             --metadata results/new_york/metadata_adjusted.tsv.xz             --output results/new_york/recency.json 2>&1 | tee logs/recency_new_york.txt
        
[Thu Nov 25 10:51:03 2021]
Finished job 43.
24 of 44 steps (55%) done
[Thu Nov 25 10:51:03 2021]
Error in rule calculate_epiweeks:
    jobid: 42
    output: results/new_york/epiweeks.json
    log: logs/calculate_epiweeks_new_york.txt (check log file(s) for error message)
    shell:
        
        python3 scripts/calculate_epiweek.py             --metadata results/new_york/metadata_adjusted.tsv.xz             --output-node-data results/new_york/epiweeks.json 2>&1 | tee logs/calculate_epiweeks_new_york.txt
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

[Thu Nov 25 10:51:03 2021]
Finished job 36.
25 of 44 steps (57%) done
[Thu Nov 25 10:51:04 2021]
Finished job 7.
26 of 44 steps (59%) done
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /vortexfs1/omics/env-bio/collaboration/env_bio_covid/ncov/.snakemake/log/2021-11-25T104420.882243.snakemake.log
